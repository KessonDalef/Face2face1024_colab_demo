{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face2face_1024px_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jmXMlffTfzA0",
        "kgxhZi0mYRph",
        "Daii-ImyguMS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KessonDalef/Face2face1024_colab_demo/blob/master/face2face_1024px_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJeLrVt4WsCZ",
        "colab_type": "text"
      },
      "source": [
        "# face2face 1024 px\n",
        "\n",
        "**A Google Colab for the face2face 1024 edited by Karol Majek to work with high resolution images.**\n",
        "\n",
        "Read his great article on Medium [HERE](https://medium.com/@karol_majek/high-resolution-face2face-with-pix2pix-1024x1024-37b90c1ca7e8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmXMlffTfzA0",
        "colab_type": "text"
      },
      "source": [
        "### Prepare "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZEVjIQeXD7K",
        "colab_type": "text"
      },
      "source": [
        "- STEP 1: Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmXzUK4_WpRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone git clone https://github.com/karolmajek/face2face-demo.git --recursive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXG9JBgGXNnp",
        "colab_type": "text"
      },
      "source": [
        "- STEP 2: Mount your Google Drive to access its files\n",
        "\n",
        "*Reasons: the reason to use Google Drive is that, first of all, you can easily import a video you want to use for the face2face from there, and then you can put your Drive folder as output for the model so if the Runtime disconnects for any reasons (it happened to me many times and I got very frustrated), you have the checkpoint saved in your Drive that you can always restore.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IiAR_pzXNAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6mB1bLXXzkL",
        "colab_type": "text"
      },
      "source": [
        "- STEP 3: Import the video from Drive\n",
        "\n",
        "*You can also right click in the menu on the eft and upload in Colab, it would get deleted in the RUntime disconnects*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7v9mzOcX_Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My video was in the main folder of Drive\n",
        "!cp \"/content/drive/My Drive/NeuralNetworks/video.mp4\" \"/content/faceface-demo/video.mp4\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgxhZi0mYRph",
        "colab_type": "text"
      },
      "source": [
        "### Generate Training Data\n",
        "\n",
        "- STEP 4: Generate the training data\n",
        "\n",
        "**FROM THE REPO:**\n",
        "  \n",
        "  INPUT: \n",
        "- **file** *is the name of the video file from which you want to create the data set.*\n",
        "- **num** *is the number of train data to be created.*\n",
        "- **landmark-model** *is the facial landmark model that is used to detect the landmarks. A pre-trained facial landmark model is provided* [here](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2). *Download it and upload by right clicking in the left panel and choose \"Upload file\"*.\n",
        "\n",
        "OUTPUT: \n",
        "\n",
        "*Two folders* **original** *and* **landmarks** *will be created.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xoy5p6htYTMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enter in the working directory\n",
        "%cd face2face-demo\n",
        "\n",
        "# And generate the data\n",
        "!python generate_train_data.py --file video.mp4 --num 400 --landmark-model shape_predictor_68_face_landmarks.dat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh5Br1MbZTqB",
        "colab_type": "text"
      },
      "source": [
        "- STEP 5: This step is not mandatory and you can skip it, we will now make a backup of the output of the previous step in our Google Drive, so in the case we want to recover from checkpoint we don't need to generate the training data, but we will copy them from our Drive\n",
        "\n",
        "*If you don't want to copy the generated data on your Drive, or you don't need/want to import the data from your Drive, you can skip steps 5 and 6.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc4h-FKYZmQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A new folder named original will be made in our Google Drive\n",
        "!cp -r \"./original\" \"/content/drive/My Drive\"\n",
        "\n",
        "# And also a new folder called landmarks will be made in our Drive\n",
        "!cp -r \"./landmarks\" \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0_czX9oaFg-",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "- STEP 6: If you want to import data from your Google Drive generated in the past, let's do it now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiLt3jyKaM_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The original images\n",
        "!cp -r \"/content/drive/My Drive/original\" \"/content/face2face-demo/\"\n",
        "\n",
        "# And the landmarks\n",
        "!cp -r \"/content/drive/My Drive/landmarks\" \"/content/face2face-demo/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5rQdTxaaz-",
        "colab_type": "text"
      },
      "source": [
        "- STEP 7: Let's copy our dataset in the pix2pix-tensorflow directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_0agHKba2w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the photo folder\n",
        "!mkdir pix2pix/photos\n",
        "\n",
        "# Copy the data from the working directory to the pix2pix dir\n",
        "!cp -r ./original pix2pix-tensorflow/photos/\n",
        "!cp -r ./landmarks pix2pix-tensroflow/photos/\n",
        "\n",
        "# Now let's remove useless data\n",
        "!rm -r ./original\n",
        "!rm -r ./landmarks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz4Xw7VcbapH",
        "colab_type": "text"
      },
      "source": [
        "- STEP 8: Let's move to the pix2pix dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeV1pu5ebcw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd pix2pix-tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roMMYjwTbfbe",
        "colab_type": "text"
      },
      "source": [
        "- STEP 9: Resize the original images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_WhuYKubi7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python tools/process.py \\\n",
        "  --input_dir photos/original \\\n",
        "  --operation resize \\\n",
        "  --size 1024 \\\n",
        "  --output_dir photos/original_resized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dltCbjIHbmLe",
        "colab_type": "text"
      },
      "source": [
        "- STEP 10: Resize landmark images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkQneItQbrlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python tools/process.py \\\n",
        "  --input_dir photos/landmarks \\\n",
        "  --operation resize \\\n",
        "  --size 1024 \\\n",
        "  --output_dir photos/landmarks_resized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLZRV4LsbtOc",
        "colab_type": "text"
      },
      "source": [
        "- STEP 11: Combine both resized original and landmark images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-F5m0iebwON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python tools/process.py \\\n",
        "  --input_dir photos/landmarks_resized \\\n",
        "  --b_dir photos/original_resized \\\n",
        "  --operation combine \\\n",
        "  --output_dir photos/combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzHj7Kmb0Ql",
        "colab_type": "text"
      },
      "source": [
        "- STEP 12: Split into train/val set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAbfth0pb3Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python tools/split.py \\\n",
        "  --dir photos/combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ps0yeTMb4e9",
        "colab_type": "text"
      },
      "source": [
        "- STEP 13: Train the model on the data\n",
        "\n",
        "**output_dir** *is the directory where the checkpoints of the model will be saved, and is the folder where you can restore. In this case, if you mounted Google Drive before, you can choose your Drive folder as output. It will be slightly slower, but we can be sure to not to lose the data in the case of disconnection.*\n",
        "\n",
        "*For more informations, check the original repo of this project:*\n",
        "https://github.com/karolmajek/face2face-demo\n",
        "\n",
        "*Uncomment the last line by removing the # if you want to use the checkpoint from Drive.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow54WFz4b9Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python pix2pix.py \\\n",
        "  --mode train \\\n",
        "  --output_dir \"/content/drive/My Drive/face2face-model\" \\\n",
        "  --max_epochs 200 \\\n",
        "  --input_dir photos/combined/train \\\n",
        "  --which_direction AtoB #\\\n",
        "#  --checkpoint \"/content/drive/My Drive/face2face-model\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d7s352dc37v",
        "colab_type": "text"
      },
      "source": [
        "- STEP 14: This step is to test the xported model and for the validation. If you are not interested in testing the model, you can skip this step.\n",
        "\n",
        "*INPUT:*\n",
        "    \n",
        "- **output_dir** *is the output folder of our test, an HTML page with some tests on the val folder*\n",
        "- **input_dir** *is the fodler where we stored the \"Val\" data for the evaluation, check the splitting step*\n",
        "- **checkpoint** *is the output_dir where we saved the checkpoint. In this case, is set to that folder in Goolgle Drive, but you can change it with what you are using.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTh6r9fSdMXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python pix2pix.py \\\n",
        "  --mode test \\\n",
        "  --output_dir face2face-test \\\n",
        "  --input_dir photos/combined/val \\\n",
        "  --checkpoint \"/content/drive/My Drive/face2face-model\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgyyvvooefU8",
        "colab_type": "text"
      },
      "source": [
        "- STEP 15: In the case you went for the test, you can zip and download the folder to open it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKjSidcAeMN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's zip the folder\n",
        "!zip -r ./face2face-test face2face-test.zip\n",
        "\n",
        "# Import file manager from COlab\n",
        "from google.colab import files\n",
        "\n",
        "# And download the zip file, it might take a while to prepare the download\n",
        "files.download(\"./face2face-test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iMivl2negzk",
        "colab_type": "text"
      },
      "source": [
        "- STEP 16: If we saved our model on Drive, let's copy it in our working directory to speed up the process\n",
        "\n",
        "*If you didn't save the model on Drive skip this passage.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UgsVe83ei-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/face2face-model\" face2face-model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snOvVUJhfUYd",
        "colab_type": "text"
      },
      "source": [
        "### Export Model\n",
        "\n",
        "- STEP 17: First, we need to reduce the trained model so that we can use an image tensor as input\n",
        "\n",
        "**FROM THE REPO:**\n",
        "\n",
        "INPUT:\n",
        "- **model-input** *is the model folder to be imported.*\n",
        "- **model-output** *is the model (reduced) folder to be exported.*\n",
        "\n",
        "OUTPUT:\n",
        "- It returns a reduced model with less weights file size than the original model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY_3nEUlfWML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ../reduce_model.py --model-input face2face-model --model-output face2face-reduced-model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDKZPaZXgZAi",
        "colab_type": "text"
      },
      "source": [
        "- STEP 18: Second, we freeze the reduced model to a single file.\n",
        "\n",
        "**FROM THE REPO:**\n",
        "\n",
        "INPUT:\n",
        "- **model-folder** *is the model folder of the reduced model.*\n",
        "\n",
        "OUTPUT:\n",
        "- *It returns a frozen model file* **frozen_model.pb** *in the model folder.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8YDr0qDgrui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ../freeze_model.py --model-folder face2face-reduced-model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daii-ImyguMS",
        "colab_type": "text"
      },
      "source": [
        "### Run the demo\n",
        "\n",
        "**This Colab is made only for the training!**\n",
        "\n",
        "Now that we trained our model, it's time to download it and use it on out machine.\n",
        "\n",
        "The best way is to follow the steps from the official repository and make it working on our machine. Run the next code to download the frozen-model. If you would like to download the entire model, uncomment the lines in orage by removing the # at the beginning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzUYHw89ht2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "# zip -r face2face-model.zip face2face-model\n",
        "# files.download(\"face-face-model.zip\")\n",
        "zip -r face2face-reduced-model.zip face2face-reduced-model\n",
        "files.download(\"face2face-reduced-model.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CwhM0h8iT5D",
        "colab_type": "text"
      },
      "source": [
        "### That's it. Hope you enjoyed!"
      ]
    }
  ]
}